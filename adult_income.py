# -*- coding: utf-8 -*-
"""Adult Income.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MFh_P3fHgI0wyEVuMjq-2gcsSMb0vGlS
"""

import pandas as pd
import numpy as np
pd.set_option('display.max_columns', None)
import logging
import Logger
logger = Logger.logger()

df = pd.read_csv('adult.csv')

df.head()

df.info()

df.describe()

df.isnull().sum()

"""Feature Engineering"""

df['workclass'] = df['workclass'].replace(to_replace=[' Local-gov',' State-gov', ' Federal-gov'], value='Government')
df['workclass'] = df['workclass'].replace(to_replace=[' ?', ' Private'], value='Private')
df['workclass'] = df['workclass'].replace(to_replace=[' Without-pay', ' Never-worked'], value='No Income')
df['workclass'] = df['workclass'].replace(to_replace=[' Self-emp-not-inc', ' Self-emp-inc'], value='Self Employeed')

df['education'] = df['education'].replace(to_replace=[' Some-college', ' Bachelors'], value='Bachelors')
df['education'] = df['education'].replace(to_replace=[' Assoc-voc', ' Assoc-acdm'], value='Associate')
df['education'] = df['education'].replace(to_replace=[' HS-grad'], value='Diploma')
df['education'] = df['education'].replace(to_replace=[" 11th", " 9th", " 7th-8th", " 5th-6th", " 10th", " 1st-4th", " 12th", " Preschool", ' Prof-school'], value='School')
df['education'] = df['education'].replace(to_replace=[' Masters', ' Doctorate'], value='Higher Studies')

df['marital-status'] = df['marital-status'].replace(to_replace=[' Married-civ-spouse', ' Married-spouse-absent', ' Married-AF-spouse'], value='Married')
df['marital-status'] = df['marital-status'].replace(to_replace=[' Never-married', ' Divorced', ' Separated', ' Widowed'], value='Not Married')

df['race'] = df['race'].replace(to_replace=[' Asian-Pac-Islander', ' Amer-Indian-Eskimo', ' Other', ' Black'], value='Other')
df['race'] = df['race'].replace(to_replace=[' White'], value='White')

df['workclass'].unique()

df['education'].unique()

df['marital-status'].unique()

df['race'].unique()

df['capital-gain'] = np.where(df['capital-gain'] <= 0, 0, 1)   #0-nogain, 1-gain
df['capital-loss'] = np.where(df['capital-loss'] <= 0, 1, 0)   #1-noloss, 0-loss
df['country'] = np.where(df['country'] == ' United-States', 'USA','Non-USA')

df['capital-gain'].unique()

df['capital-loss'].unique()

df['country'].unique()

x = pd.get_dummies(df[['workclass', 'marital-status', 'race', 'sex', 'hours-per-week', 'country', 'salary']], drop_first=True , prefix=None)

df = pd.concat([df, x], axis=1)

edu = ['Higher Studies', 'Bachelors', 'Associate', 'Prof-school', 'Diploma', 'School']

from sklearn.preprocessing import OrdinalEncoder

order_label = {'Higher Studies':6, 'Bachelors':5, 'Associate':4, 'Prof-school':3, 'Diploma':2, 'School':1}

df['education_ord_lbl'] = df['education'].map(order_label)

df

df.drop(['education', 'education-num', 'occupation', 'relationship', 'workclass', 'marital-status', 'race', 'sex',
                  'hours-per-week', 'country', 'salary'], axis=1, inplace=True)

df

df['fnlwgt'] = df['fnlwgt'].mask(df['fnlwgt'] > df['fnlwgt'].quantile(0.90), df['fnlwgt'].mean())

df['fnlwgt']

x = df.drop('salary_ >50K', axis=1)

y = df['salary_ >50K']

"""Random Over Sampling"""

from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler()
x, y = ros.fit_resample(x, y)

"""Splitting data"""

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=100)

"""Model Building"""

from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import BaggingClassifier
import numpy as np
from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from sklearn import metrics
from sklearn.model_selection import RandomizedSearchCV
from sklearn.neighbors import KNeighborsClassifier
import pickle
import bz2
from sklearn.ensemble import GradientBoostingClassifier
import _pickle as cPickle
import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)

"""Decision Tree"""

dtc = DecisionTreeClassifier()
dtc.fit(x_train, y_train)

y_pred = dtc.predict(x_test)
test_acc = metrics.accuracy_score(y_test, y_pred)
x_pred = dtc.predict(x_train)
train_acc = metrics.accuracy_score(y_train, x_pred)

test_acc, train_acc

"""Random Forest"""

rfc = RandomForestClassifier()
rfc.fit(x_train, y_train)

y_pred = rfc.predict(x_test)
test_acc = metrics.accuracy_score(y_test, y_pred)
x_pred = rfc.predict(x_train)
train_acc = metrics.accuracy_score(y_train, x_pred)

test_acc, train_acc

"""Bagging"""

bc = BaggingClassifier()
bc.fit(x_train, y_train)

y_pred = bc.predict(x_test)
test_acc = metrics.accuracy_score(y_test, y_pred)
x_pred = bc.predict(x_train)
train_acc = metrics.accuracy_score(y_train, x_pred)

test_acc, train_acc

"""Xgboost"""

xgbc = XGBClassifier()
xgbc.fit(x_train, y_train)

y_pred = xgbc.predict(x_test)
test_acc = metrics.accuracy_score(y_test, y_pred)
x_pred = xgbc.predict(x_train)
train_acc = metrics.accuracy_score(y_train, x_pred)

test_acc, train_acc

"""ExtraTree Classifier"""

etc = ExtraTreesClassifier()
etc.fit(x_train, y_train)

y_pred = etc.predict(x_test)
test_acc = metrics.accuracy_score(y_test, y_pred)
x_pred = etc.predict(x_train)
train_acc = metrics.accuracy_score(y_train, x_pred)

test_acc, train_acc

"""Hyperparameter Tuning

Tuning for Decision Tree
"""

params = {
    'criterion': ['gini', 'entropy'],
                'max_depth': [i for i in range(1, 20, 1)],
                'min_samples_leaf': [i for i in range(1, 20, 1)],
                'min_samples_split': [i for i in range(2, 20, 1)],
                'max_features': ['sqrt', 'log2', None],
                'ccp_alpha': [float(i) for i in np.linspace(0, 1, 10)]
    
}

dtc_random = RandomizedSearchCV(estimator=dtc, param_distributions=params, cv=10, n_iter=100, n_jobs=6, random_state=100, verbose=True)
dtc_random.fit(x_train, y_train)
dtc_best = dtc_random.best_params_
print(dtc_best)

dtc_clf = DecisionTreeClassifier(criterion=dtc_best['criterion'], 
                                 max_depth = dtc_best['max_depth'], 
                                 min_samples_leaf = dtc_best['min_samples_leaf'], 
                                 min_samples_split = dtc_best['min_samples_split'], 
                                 max_features = dtc_best['max_features'], 
                                 ccp_alpha = dtc_best['ccp_alpha'])
dtc_clf.fit(x_train, y_train)

y_pred_clf = dtc_clf.predict(x_test)
test_acc_clf = metrics.accuracy_score(y_test, y_pred_clf)
x_pred_clf = dtc_clf.predict(x_train)
train_acc_clf = metrics.accuracy_score(y_train, x_pred_clf)
test_acc_clf, train_acc_clf

"""Tuning for Random Forest"""

params = {
        'n_estimators': [int(x) for x in np.linspace(start=100,stop=1000,num=30)],
        'criterion': ['gini', 'entropy'],
        'min_samples_leaf': [int(x) for x in range(1, 25, 1)],
        'min_samples_split': [int(x) for x in range(2, 50, 1)],
        'max_features': ['sqrt', 'log2', None],
        'max_depth': [int(x) for x in range(1,30,5)]
}

rfc_random = RandomizedSearchCV(estimator=rfc, param_distributions=params, n_iter=20, cv=3, n_jobs=-1, verbose=True, random_state=100)
rfc_random.fit(x_train, y_train)
rfc_best = rfc_random.best_params_
print(rfc_best)

rfc_clf = RandomForestClassifier(criterion=rfc_best['criterion'], n_estimators=rfc_best['n_estimators'],
                                           min_samples_leaf=rfc_best['min_samples_leaf'],
                                           min_samples_split=rfc_best['min_samples_split'],
                                           max_features=rfc_best['max_features'], max_depth=rfc_best['max_depth'])
rfc_clf.fit(x_train, y_train)

y_pred_clf = rfc_clf.predict(x_test)
test_acc_clf = metrics.accuracy_score(y_test, y_pred_clf)
x_pred_clf = rfc_clf.predict(x_train)
train_acc_clf = metrics.accuracy_score(y_train, x_pred_clf)
test_acc_clf, train_acc_clf

"""Tuning for ExtraTree Classifier"""

params = {
        'n_estimators': [int(i) for i in range(100, 1000, 100)],
        'criterion': ['gini', 'entropy'],
        'min_samples_leaf': [int(x) for x in range(1, 50, 1)],
        'min_samples_split': [int(x) for x in range(2, 50, 1)],
        'max_features': ['sqrt', 'log2', None]
}
etc_grid = RandomizedSearchCV(estimator=etc, param_distributions=params, n_iter=50, cv=3, n_jobs=-1, verbose=True, random_state=100)
etc_grid.fit(x_train, y_train)
etc_best = etc_grid.best_params_
print(etc_best)

etc_clf = ExtraTreesClassifier(criterion=etc_best['criterion'], 
                               n_estimators=etc_best['n_estimators'], 
                               min_samples_leaf=etc_best['min_samples_leaf'], 
                               min_samples_split=etc_best['min_samples_split'], 
                               max_features=etc_best['max_features'])
etc_clf.fit(x_train, y_train)

y_pred_clf = etc_clf.predict(x_test)
test_acc_clf = metrics.accuracy_score(y_test, y_pred_clf)
x_pred_clf = etc_clf.predict(x_train)
train_acc_clf = metrics.accuracy_score(y_train, x_pred_clf)
test_acc_clf, train_acc_clf

"""Tuning for xgboost"""

params = {
        "learning_rate": [float(i) for i in np.linspace(0.001,0.2,20)],
        #"max_depth": [3, 4, 5, 6, 8, 10, 12, 15],
        "min_child_weight": [1, 3, 5, 7],
        "gamma": [float(i) for i in np.linspace(0.1,1,10)],
        "colsample_bytree": [0.2,0.3, 0.4, 0.5, 0.7,0.8,0.9]
}
xgbc_grid = RandomizedSearchCV(estimator=xgbc, param_distributions=params, cv=3, n_iter=50, n_jobs=6, verbose=True, random_state=75)
xgbc_grid.fit(x_train, y_train)
xgbc_best = xgbc_grid.best_params_
print(xgbc_best)

xgb_clf = XGBClassifier(learning_rate=xgbc_best['learning_rate'], 
                        min_child_weight=xgbc_best['min_child_weight'], 
                        gamma=xgbc_best['gamma'], 
                        colsample_bytree=xgbc_best['colsample_bytree'])
xgb_clf.fit(x_train, y_train)

y_pred_clf = xgb_clf.predict(x_test)
test_acc_clf = metrics.accuracy_score(y_test, y_pred_clf)
x_pred_clf = xgb_clf.predict(x_train)
train_acc_clf = metrics.accuracy_score(y_train, x_pred_clf)
test_acc_clf, train_acc_clf

"""Tuning for Bagging"""

from sklearn.svm import SVC
knc = KNeighborsClassifier()
knc.fit(x_train, y_train)
# svc = SVC()
# svc.fit(x_train, y_train)
dtc1 = DecisionTreeClassifier()
dtc1.fit(x_train, y_train)
            
params = {
        'base_estimator': [knc, dtc1],
        'n_estimators': [i for i in range(100, 300, 10)]
}

bc_grid = RandomizedSearchCV(estimator=bc, param_distributions=params, n_iter=25, cv=2, n_jobs=-1, random_state=75, verbose=True)
bc_grid.fit(x_train, y_train)
bc_best = bc_grid.best_estimator_
print(bc_best)

y_pred_clf = bc_grid.predict(x_test)
test_acc_clf = metrics.accuracy_score(y_test, y_pred_clf)
x_pred_clf = bc_grid.predict(x_train)
train_acc_clf = metrics.accuracy_score(y_train, x_pred_clf)
test_acc_clf, train_acc_clf



"""Saving Model"""

pickle.dump(etc_clf, open('extratreemodel.sav', 'wb'))

